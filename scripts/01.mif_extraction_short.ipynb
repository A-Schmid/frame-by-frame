{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mif_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd011d64e2c365a879321da0be5cd1df50cb6c046188e6a9b5496d5c3da51477abc",
      "display_name": "Python 3.8.5 64-bit ('torch': conda)"
    },
    "accelerator": "GPU",
    "metadata": {
      "interpreter": {
        "hash": "11d64e2c365a879321da0be5cd1df50cb6c046188e6a9b5496d5c3da51477abc"
      }
    }
  },
  "cells": [
    {
      "source": [
        "# Most Informative Frame (MIF) extraction\n",
        "\n",
        "**Task:**\n",
        "\n",
        "Having a given set of videos, perform frame-wise classification of these videos using a pre-trained model.\n",
        "In this case, the model is a ResNet50 pretrained on Moments in Time v1 (MiTv1)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Preparation"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nGZoEX8iAfR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b52bdce8-ec35-4e17-fa9c-2f2733ec665a"
      },
      "source": [
        "# Install decord if necessary\n",
        "# AB: Why? What does it do?\n",
        "# AB: Generally check out the comments in '01.mif_extraction_exploratory' and copy some infor here as well; this should be sufficient\n",
        "!pip install --upgrade decord"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting decord\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/5e/e2be6a3a3a46275059574d9c6a1d422aa6c7c3cbf6614939b8a3c3f8f2d5/decord-0.5.2-py3-none-manylinux2010_x86_64.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1MB 250kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from decord) (1.19.5)\n",
            "Installing collected packages: decord\n",
            "Successfully installed decord-0.5.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "source": [
        "#### Load pretrained model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzscOB3DeenY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b828f5-b9b0-4c7b-8927-8e3a9c62deb5"
      },
      "source": [
        "#%% Define and load model\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "with_cuda = False\n",
        "# Define and load model\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "with_cuda = False\n",
        "path_prefix = Path('..')\n",
        "\n",
        "if with_cuda:\n",
        "    resnet50 = models.resnet50(pretrained=False, progress=True, num_classes=339).to('cuda')\n",
        "else:\n",
        "    resnet50 = models.resnet50(pretrained=False, progress=True, num_classes=339)\n",
        "\n",
        "# AB: where from?\n",
        "# Load pretrained weights (MiTv1)\n",
        "#path_model = Path('/content/drive/MyDrive/resnet50_moments-fd0c4436.pth')\n",
        "path_model = path_prefix / 'models/weights/resnet50_moments-fd0c4436.pth'\n",
        "resnet50.load_state_dict(torch.load(path_model))\n",
        "\n",
        "# AB: source / structure\n",
        "# Evaluation mode\n",
        "resnet50.eval()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=339, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "#### Define transformations"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8neHBNYeyIG"
      },
      "source": [
        "#%% Transformations\n",
        "import torchvision.transforms as transforms\n",
        "transformation = transforms.Compose([\n",
        "                                     transforms.ToPILImage(mode='RGB'), # required if the input image is a nd.array\n",
        "                                     transforms.Resize(224), # To be changed to rescale to keep the aspect ration?\n",
        "                                     transforms.CenterCrop((224, 224)), # AB: cropped to 224x224px\n",
        "                                     transforms.ToTensor(), # AB: creates Tensor (?)\n",
        "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                          std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "#### Load categories"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgpkWhHQe2Zk"
      },
      "source": [
        "# %% Load categories\n",
        "# AB: How/Where stored? Just the names/labels of the categories? --> make clear\n",
        "path_labels = path_prefix / 'models/labels/category_momentsv1.txt'\n",
        "\n",
        "def load_categories():\n",
        "    \"\"\"Load categories.\"\"\"\n",
        "    with open(path_labels) as f:\n",
        "        return [line.rstrip() for line in f.readlines()]\n",
        "\n",
        "# load categories\n",
        "categories = load_categories()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "source": [
        "#### Load videos"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3rEchfte5wa",
        "outputId": "6fffc65a-4dcc-4ca3-8e0a-748ced6da42f"
      },
      "source": [
        "#%% Sweep through files in subfolders of path_input\n",
        "import os\n",
        "path_input = path_prefix / 'input_data/TRIMMING/MP4s'\n",
        "\n",
        "l_videos = []\n",
        "for path, subdirs, files in os.walk(path_input):\n",
        "  for name in files:\n",
        "    if name[-3:] == 'mp4':\n",
        "      l_videos.append([path.split('/')[-1],   # category\n",
        "                       name])                 # file name\n",
        "    else:\n",
        "      print('Ignored: ', name)\n",
        "\n",
        "if l_videos:\n",
        "  l_videos = sorted(l_videos)\n",
        "print('Total nr. of MP4s: ', len(l_videos))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignored:  .DS_Store\nTotal nr. of MP4s:  834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['aiming', 'yt-0gwUV4Ze-Hs_390.mp4'], ['aiming', 'yt-0qYbATyHm2A_59.mp4'], ['aiming', 'yt-iVSy96zolvw_23.mp4'], ['applauding', 'yt-06tUmXhgnSY_4.mp4'], ['applauding', 'yt-A70byjNkwdA_4.mp4'], ['applauding', 'yt-e9YwItc0qUc_5.mp4'], ['arresting', 'yt-A8r4MK3R4PI_175.mp4'], ['arresting', 'yt-aAVfUYxx12g_18.mp4'], ['arresting', 'yt-wY-LUhSZtv8_16.mp4'], ['ascending', 'vine_R-Mah0Jqm9OXZ_1.mp4']]\n"
          ]
        }
      ],
      "source": [
        "print(l_videos[:10])"
      ]
    },
    {
      "source": [
        "## TRUE class & MIF only\n",
        "\n",
        "Extract prediction accuracy for the TRUE category on MIFs (most informative frame, i.e. frame w/ highest prediction accuracy)\n",
        "\n",
        "**Output**\n",
        "\n",
        "```\n",
        "pandas.DataFrame w/ columns ['category', 'fname', 'mif_idx', 'softmax[category]']\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Extract prediction accuracy for the TRUE category on MIFs (most informative frame, i.e. frame w/ highest prediction accuracy)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTF7ta-Ge9qL",
        "outputId": "32d09d66-63fe-4c08-f020-0f665686efdb"
      },
      "source": [
        "# %% Sweep through videos\n",
        "import time\n",
        "import decord\n",
        "decord.bridge.set_bridge('native') # Seems to be the fastest option\n",
        "from decord import cpu, gpu\n",
        "from decord import VideoReader\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "l_mifs = []\n",
        "\n",
        "# Iterate over entries in l_videos:\n",
        "for j in range(len(l_videos)):\n",
        "  # Verbose\n",
        "  if j%50 == 0:\n",
        "    print(f'{j}/{len(l_videos)}')\n",
        "  \n",
        "  \n",
        "  # Define path\n",
        "  category, file_name = l_videos[j]\n",
        "  cat_idx = categories.index(category)\n",
        "  path_input_file = str(path_input / category/ file_name)\n",
        "  \n",
        "  print(category, cat_idx)\n",
        "  \"\"\"\n",
        "  # Load video with Decord.VideoReader\n",
        "  vr = VideoReader(path_input_file)\n",
        "  video_frames = vr.get_batch(range(0, len(vr), 1)).asnumpy()\n",
        "  \n",
        "  # Define empty array for accuracies\n",
        "  pred_accuracies = np.zeros((video_frames.shape[0], ))\n",
        "\n",
        "  # Iterate over frames\n",
        "  for i in range(video_frames.shape[0]):\n",
        "    if with_cuda:\n",
        "      input = transformation(video_frames[i]).to('cuda')\n",
        "    else:\n",
        "      input = transformation(video_frames[i])\n",
        "    \n",
        "    # Classification:\n",
        "    logit = resnet50.forward(input.unsqueeze(0))      # extract output to given input \n",
        "    h_x = F.softmax(logit, 1).data.squeeze()[cat_idx] # transform to softmax\n",
        "    pred_accuracies[i]= h_x\n",
        "\n",
        "  \n",
        "  # Append to output list\n",
        "  l_mifs.append([category, file_name,\n",
        "                 np.argmax(pred_accuracies),\n",
        "                 pred_accuracies[np.argmax(pred_accuracies)]])\n",
        "   \"\"\"\n",
        "stop = time.time()\n",
        "duration = stop-start\n",
        "print(f'\\nTime elapsed:: {duration:.4f}s (~ {duration/j:.2f}s per file)')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/834\n",
            "aiming 35\n",
            "aiming 35\n",
            "aiming 35\n",
            "applauding 206\n",
            "applauding 206\n",
            "applauding 206\n",
            "arresting 260\n",
            "arresting 260\n",
            "arresting 260\n",
            "ascending 328\n",
            "ascending 328\n",
            "ascending 328\n",
            "asking 50\n",
            "asking 50\n",
            "asking 50\n",
            "assembling 217\n",
            "assembling 217\n",
            "assembling 217\n",
            "attacking 174\n",
            "attacking 174\n",
            "attacking 174\n",
            "autographing 267\n",
            "autographing 267\n",
            "autographing 267\n",
            "baking 273\n",
            "baking 273\n",
            "baking 273\n",
            "balancing 113\n",
            "balancing 113\n",
            "balancing 113\n",
            "baptizing 198\n",
            "baptizing 198\n",
            "baptizing 198\n",
            "barbecuing 231\n",
            "barbecuing 231\n",
            "barbecuing 231\n",
            "bathing 104\n",
            "bathing 104\n",
            "bathing 104\n",
            "bending 143\n",
            "bending 143\n",
            "bending 143\n",
            "bicycling 152\n",
            "bicycling 152\n",
            "bicycling 152\n",
            "biting 294\n",
            "biting 294\n",
            "biting 294\n",
            "blocking 317\n",
            "blocking 317\n",
            "50/834\n",
            "blocking 317\n",
            "blowing 278\n",
            "blowing 278\n",
            "blowing 278\n",
            "boarding 20\n",
            "boarding 20\n",
            "boarding 20\n",
            "boating 196\n",
            "boating 196\n",
            "boating 196\n",
            "bouncing 290\n",
            "bouncing 290\n",
            "bouncing 290\n",
            "bowing 263\n",
            "bowing 263\n",
            "bowing 263\n",
            "boxing 144\n",
            "boxing 144\n",
            "boxing 144\n",
            "breaking 18\n",
            "breaking 18\n",
            "breaking 18\n",
            "brushing 117\n",
            "brushing 117\n",
            "brushing 117\n",
            "bubbling 102\n",
            "bubbling 102\n",
            "bubbling 102\n",
            "building 92\n",
            "building 92\n",
            "building 92\n",
            "burying 3\n",
            "burying 3\n",
            "burying 3\n",
            "buttoning 29\n",
            "buttoning 29\n",
            "buttoning 29\n",
            "buying 151\n",
            "buying 151\n",
            "buying 151\n",
            "calling 207\n",
            "calling 207\n",
            "calling 207\n",
            "camping 52\n",
            "camping 52\n",
            "camping 52\n",
            "carrying 31\n",
            "carrying 31\n",
            "carrying 31\n",
            "carving 178\n",
            "100/834\n",
            "carving 178\n",
            "carving 178\n",
            "catching 307\n",
            "catching 307\n",
            "catching 307\n",
            "celebrating 312\n",
            "celebrating 312\n",
            "celebrating 312\n",
            "cheering 150\n",
            "cheering 150\n",
            "cheering 150\n",
            "cheerleading 96\n",
            "cheerleading 96\n",
            "cheerleading 96\n",
            "chewing 259\n",
            "chewing 259\n",
            "chewing 259\n",
            "chopping 204\n",
            "chopping 204\n",
            "chopping 204\n",
            "clapping 0\n",
            "clapping 0\n",
            "clapping 0\n",
            "clawing 142\n",
            "clawing 142\n",
            "clawing 142\n",
            "cleaning 279\n",
            "cleaning 279\n",
            "cleaning 279\n",
            "clearing 189\n",
            "clearing 189\n",
            "clearing 189\n",
            "climbing 182\n",
            "climbing 182\n",
            "climbing 182\n",
            "clipping 47\n",
            "clipping 47\n",
            "clipping 47\n",
            "closing 187\n",
            "closing 187\n",
            "closing 187\n",
            "coaching 16\n",
            "coaching 16\n",
            "coaching 16\n",
            "combing 280\n",
            "combing 280\n",
            "combing 280\n",
            "combusting 283\n",
            "combusting 283\n",
            "combusting 283\n",
            "150/834\n",
            "competing 23\n",
            "competing 23\n",
            "competing 23\n",
            "constructing 55\n",
            "constructing 55\n",
            "constructing 55\n",
            "cooking 127\n",
            "cooking 127\n",
            "cooking 127\n",
            "coughing 110\n",
            "coughing 110\n",
            "coughing 110\n",
            "cracking 61\n",
            "cracking 61\n",
            "cracking 61\n",
            "crafting 243\n",
            "crafting 243\n",
            "crafting 243\n",
            "cramming 330\n",
            "cramming 330\n",
            "cramming 330\n",
            "crawling 215\n",
            "crawling 215\n",
            "crawling 215\n",
            "crouching 36\n",
            "crouching 36\n",
            "crouching 36\n",
            "crushing 118\n",
            "crushing 118\n",
            "crushing 118\n",
            "crying 161\n",
            "crying 161\n",
            "crying 161\n",
            "cuddling 9\n",
            "cuddling 9\n",
            "cuddling 9\n",
            "cutting 164\n",
            "cutting 164\n",
            "cutting 164\n",
            "dancing 168\n",
            "dancing 168\n",
            "dancing 168\n",
            "descending 222\n",
            "descending 222\n",
            "descending 222\n",
            "destroying 22\n",
            "destroying 22\n",
            "destroying 22\n",
            "digging 148\n",
            "digging 148\n",
            "200/834\n",
            "digging 148\n",
            "dining 94\n",
            "dining 94\n",
            "dining 94\n",
            "dipping 202\n",
            "dipping 202\n",
            "dipping 202\n",
            "discussing 133\n",
            "discussing 133\n",
            "discussing 133\n",
            "diving 90\n",
            "diving 90\n",
            "diving 90\n",
            "draining 308\n",
            "draining 308\n",
            "draining 308\n",
            "drawing 107\n",
            "drawing 107\n",
            "drawing 107\n",
            "dressing 180\n",
            "dressing 180\n",
            "dressing 180\n",
            "drilling 234\n",
            "drilling 234\n",
            "drilling 234\n",
            "drinking 7\n",
            "drinking 7\n",
            "drinking 7\n",
            "driving 78\n",
            "driving 78\n",
            "driving 78\n",
            "drying 160\n",
            "drying 160\n",
            "drying 160\n",
            "dunking 116\n",
            "dunking 116\n",
            "dunking 116\n",
            "dusting 291\n",
            "dusting 291\n",
            "dusting 291\n",
            "eating 166\n",
            "eating 166\n",
            "eating 166\n",
            "emptying 154\n",
            "emptying 154\n",
            "emptying 154\n",
            "entering 224\n",
            "entering 224\n",
            "entering 224\n",
            "exercising 335\n",
            "250/834\n",
            "exercising 335\n",
            "exercising 335\n",
            "exiting 303\n",
            "exiting 303\n",
            "exiting 303\n",
            "extinguishing 205\n",
            "extinguishing 205\n",
            "extinguishing 205\n",
            "falling 223\n",
            "falling 223\n",
            "falling 223\n",
            "feeding 153\n",
            "feeding 153\n",
            "feeding 153\n",
            "fighting 229\n",
            "fighting 229\n",
            "fighting 229\n",
            "filling 81\n",
            "filling 81\n",
            "filling 81\n",
            "filming 77\n",
            "filming 77\n",
            "filming 77\n",
            "fishing 192\n",
            "fishing 192\n",
            "fishing 192\n",
            "flicking 27\n",
            "flicking 27\n",
            "flicking 27\n",
            "flipping 45\n",
            "flipping 45\n",
            "flipping 45\n",
            "floating 95\n",
            "floating 95\n",
            "floating 95\n",
            "flying 216\n",
            "flying 216\n",
            "flying 216\n",
            "folding 300\n",
            "folding 300\n",
            "folding 300\n",
            "frowning 186\n",
            "frowning 186\n",
            "frowning 186\n",
            "frying 163\n",
            "frying 163\n",
            "frying 163\n",
            "fueling 266\n",
            "fueling 266\n",
            "fueling 266\n",
            "300/834\n",
            "gardening 135\n",
            "gardening 135\n",
            "gardening 135\n",
            "giving 89\n",
            "giving 89\n",
            "giving 89\n",
            "grilling 239\n",
            "grilling 239\n",
            "grilling 239\n",
            "gripping 146\n",
            "gripping 146\n",
            "gripping 146\n",
            "grooming 261\n",
            "grooming 261\n",
            "grooming 261\n",
            "guarding 296\n",
            "guarding 296\n",
            "guarding 296\n",
            "hammering 30\n",
            "hammering 30\n",
            "hammering 30\n",
            "handcuffing 311\n",
            "handcuffing 311\n",
            "handcuffing 311\n",
            "handwriting 79\n",
            "handwriting 79\n",
            "handwriting 79\n",
            "hanging 325\n",
            "hanging 325\n",
            "hanging 325\n",
            "hiking 86\n",
            "hiking 86\n",
            "hiking 86\n",
            "hitchhiking 60\n",
            "hitchhiking 60\n",
            "hitchhiking 60\n",
            "hitting 101\n",
            "hitting 101\n",
            "hitting 101\n",
            "howling 321\n",
            "howling 321\n",
            "howling 321\n",
            "hugging 91\n",
            "hugging 91\n",
            "hugging 91\n",
            "hunting 188\n",
            "hunting 188\n",
            "hunting 188\n",
            "inflating 181\n",
            "inflating 181\n",
            "350/834\n",
            "inflating 181\n",
            "injecting 218\n",
            "injecting 218\n",
            "injecting 218\n",
            "interviewing 257\n",
            "interviewing 257\n",
            "interviewing 257\n",
            "jogging 313\n",
            "jogging 313\n",
            "jogging 313\n",
            "juggling 253\n",
            "juggling 253\n",
            "juggling 253\n",
            "jumping 68\n",
            "jumping 68\n",
            "jumping 68\n",
            "kicking 75\n",
            "kicking 75\n",
            "kicking 75\n",
            "kissing 67\n",
            "kissing 67\n",
            "kissing 67\n",
            "kneeling 115\n",
            "kneeling 115\n",
            "kneeling 115\n",
            "knocking 244\n",
            "knocking 244\n",
            "knocking 244\n",
            "laughing 214\n",
            "laughing 214\n",
            "laughing 214\n",
            "lecturing 167\n",
            "lecturing 167\n",
            "lecturing 167\n",
            "licking 74\n",
            "licking 74\n",
            "licking 74\n",
            "lifting 298\n",
            "lifting 298\n",
            "lifting 298\n",
            "loading 336\n",
            "loading 336\n",
            "loading 336\n",
            "locking 42\n",
            "locking 42\n",
            "locking 42\n",
            "marching 65\n",
            "marching 65\n",
            "marching 65\n",
            "marrying 212\n",
            "400/834\n",
            "marrying 212\n",
            "marrying 212\n",
            "massaging 309\n",
            "massaging 309\n",
            "massaging 309\n",
            "measuring 301\n",
            "measuring 301\n",
            "measuring 301\n",
            "mopping 145\n",
            "mopping 145\n",
            "mopping 145\n",
            "officiating 137\n",
            "officiating 137\n",
            "officiating 137\n",
            "opening 251\n",
            "opening 251\n",
            "opening 251\n",
            "operating 220\n",
            "operating 220\n",
            "operating 220\n",
            "packaging 191\n",
            "packaging 191\n",
            "packaging 191\n",
            "packing 221\n",
            "packing 221\n",
            "packing 221\n",
            "painting 233\n",
            "painting 233\n",
            "painting 233\n",
            "parading 322\n",
            "parading 322\n",
            "parading 322\n",
            "paying 165\n",
            "paying 165\n",
            "paying 165\n",
            "pedaling 54\n",
            "pedaling 54\n",
            "pedaling 54\n",
            "peeling 171\n",
            "peeling 171\n",
            "peeling 171\n",
            "performing 136\n",
            "performing 136\n",
            "performing 136\n",
            "photographing 138\n",
            "photographing 138\n",
            "photographing 138\n",
            "picking 72\n",
            "picking 72\n",
            "picking 72\n",
            "450/834\n",
            "piloting 337\n",
            "piloting 337\n",
            "piloting 337\n",
            "planting 326\n",
            "planting 326\n",
            "planting 326\n",
            "playing+fun 51\n",
            "playing+fun 51\n",
            "playing+fun 51\n",
            "playing+music 122\n",
            "playing+music 122\n",
            "playing+music 122\n",
            "playing+sports 199\n",
            "playing+sports 199\n",
            "playing+sports 199\n",
            "plugging 53\n",
            "plugging 53\n",
            "plugging 53\n",
            "plunging 238\n",
            "plunging 238\n",
            "plunging 238\n",
            "pointing 88\n",
            "pointing 88\n",
            "pointing 88\n",
            "poking 333\n",
            "poking 333\n",
            "poking 333\n",
            "pouring 28\n",
            "pouring 28\n",
            "pouring 28\n",
            "praying 1\n",
            "praying 1\n",
            "praying 1\n",
            "preaching 11\n",
            "preaching 11\n",
            "preaching 11\n",
            "pressing 84\n",
            "pressing 84\n",
            "pressing 84\n",
            "protesting 108\n",
            "protesting 108\n",
            "protesting 108\n",
            "pulling 33\n",
            "pulling 33\n",
            "pulling 33\n",
            "punching 235\n",
            "punching 235\n",
            "punching 235\n",
            "punting 120\n",
            "punting 120\n",
            "500/834\n",
            "punting 120\n",
            "pushing 225\n",
            "pushing 225\n",
            "pushing 225\n",
            "racing 282\n",
            "racing 282\n",
            "racing 282\n",
            "rafting 114\n",
            "rafting 114\n",
            "rafting 114\n",
            "raising 105\n",
            "raising 105\n",
            "raising 105\n",
            "reading 184\n",
            "reading 184\n",
            "reading 184\n",
            "repairing 272\n",
            "repairing 272\n",
            "repairing 272\n",
            "riding 203\n",
            "riding 203\n",
            "riding 203\n",
            "rinsing 109\n",
            "rinsing 109\n",
            "rinsing 109\n",
            "rising 213\n",
            "rising 213\n",
            "rising 213\n",
            "rocking 49\n",
            "rocking 49\n",
            "rocking 49\n",
            "rolling 200\n",
            "rolling 200\n",
            "rolling 200\n",
            "rowing 262\n",
            "rowing 262\n",
            "rowing 262\n",
            "rubbing 119\n",
            "rubbing 119\n",
            "rubbing 119\n",
            "running 21\n",
            "running 21\n",
            "running 21\n",
            "sailing 98\n",
            "sailing 98\n",
            "sailing 98\n",
            "saluting 265\n",
            "saluting 265\n",
            "saluting 265\n",
            "sanding 185\n",
            "550/834\n",
            "sanding 185\n",
            "sanding 185\n",
            "sawing 226\n",
            "sawing 226\n",
            "sawing 226\n",
            "scratching 62\n",
            "scratching 62\n",
            "scratching 62\n",
            "screwing 58\n",
            "screwing 58\n",
            "screwing 58\n",
            "scrubbing 310\n",
            "scrubbing 310\n",
            "scrubbing 310\n",
            "serving 130\n",
            "serving 130\n",
            "serving 130\n",
            "sewing 46\n",
            "sewing 46\n",
            "sewing 46\n",
            "shaking 132\n",
            "shaking 132\n",
            "shaking 132\n",
            "shaving 211\n",
            "shaving 211\n",
            "shaving 211\n",
            "shooting 255\n",
            "shooting 255\n",
            "shooting 255\n",
            "shopping 289\n",
            "shopping 289\n",
            "shopping 289\n",
            "shouting 85\n",
            "shouting 85\n",
            "shouting 85\n",
            "shoveling 25\n",
            "shoveling 25\n",
            "shoveling 25\n",
            "shredding 183\n",
            "shredding 183\n",
            "shredding 183\n",
            "shrugging 59\n",
            "shrugging 59\n",
            "shrugging 59\n",
            "signing 271\n",
            "signing 271\n",
            "signing 271\n",
            "singing 99\n",
            "singing 99\n",
            "singing 99\n",
            "600/834\n",
            "sitting 106\n",
            "sitting 106\n",
            "sitting 106\n",
            "skating 232\n",
            "skating 232\n",
            "skating 232\n",
            "sketching 156\n",
            "sketching 156\n",
            "sketching 156\n",
            "skiing 275\n",
            "skiing 275\n",
            "skiing 275\n",
            "skipping 38\n",
            "skipping 38\n",
            "skipping 38\n",
            "slapping 8\n",
            "slapping 8\n",
            "slapping 8\n",
            "sleeping 10\n",
            "sleeping 10\n",
            "sleeping 10\n",
            "slicing 112\n",
            "slicing 112\n",
            "slicing 112\n",
            "sliding 76\n",
            "sliding 76\n",
            "sliding 76\n",
            "slipping 56\n",
            "slipping 56\n",
            "slipping 56\n",
            "smashing 111\n",
            "smashing 111\n",
            "smashing 111\n",
            "smelling 227\n",
            "smelling 227\n",
            "smelling 227\n",
            "smiling 318\n",
            "smiling 318\n",
            "smiling 318\n",
            "smoking 274\n",
            "smoking 274\n",
            "smoking 274\n",
            "snapping 293\n",
            "snapping 293\n",
            "snapping 293\n",
            "sneezing 44\n",
            "sneezing 44\n",
            "sneezing 44\n",
            "sniffing 256\n",
            "sniffing 256\n",
            "650/834\n",
            "sniffing 256\n",
            "snuggling 288\n",
            "snuggling 288\n",
            "snuggling 288\n",
            "sowing 139\n",
            "sowing 139\n",
            "sowing 139\n",
            "speaking 327\n",
            "speaking 327\n",
            "speaking 327\n",
            "spilling 193\n",
            "spilling 193\n",
            "spilling 193\n",
            "spinning 162\n",
            "spinning 162\n",
            "spinning 162\n",
            "spitting 201\n",
            "spitting 201\n",
            "spitting 201\n",
            "splashing 73\n",
            "splashing 73\n",
            "splashing 73\n",
            "spraying 14\n",
            "spraying 14\n",
            "spraying 14\n",
            "spreading 281\n",
            "spreading 281\n",
            "spreading 281\n",
            "sprinkling 197\n",
            "sprinkling 197\n",
            "sprinkling 197\n",
            "sprinting 324\n",
            "sprinting 324\n",
            "sprinting 324\n",
            "squatting 34\n",
            "squatting 34\n",
            "squatting 34\n",
            "squinting 306\n",
            "squinting 306\n",
            "squinting 306\n",
            "stacking 159\n",
            "stacking 159\n",
            "stacking 159\n",
            "standing 157\n",
            "standing 157\n",
            "standing 157\n",
            "stealing 83\n",
            "stealing 83\n",
            "stealing 83\n",
            "steering 80\n",
            "700/834\n",
            "steering 80\n",
            "steering 80\n",
            "stirring 66\n",
            "stirring 66\n",
            "stirring 66\n",
            "stitching 13\n",
            "stitching 13\n",
            "stitching 13\n",
            "stomping 258\n",
            "stomping 258\n",
            "stomping 258\n",
            "stretching 304\n",
            "stretching 304\n",
            "stretching 304\n",
            "studying 129\n",
            "studying 129\n",
            "studying 129\n",
            "submerging 17\n",
            "submerging 17\n",
            "submerging 17\n",
            "surfing 32\n",
            "surfing 32\n",
            "surfing 32\n",
            "sweeping 57\n",
            "sweeping 57\n",
            "sweeping 57\n",
            "swimming 286\n",
            "swimming 286\n",
            "swimming 286\n",
            "swinging 177\n",
            "swinging 177\n",
            "swinging 177\n",
            "taping 305\n",
            "taping 305\n",
            "taping 305\n",
            "tapping 37\n",
            "tapping 37\n",
            "tapping 37\n",
            "tattooing 319\n",
            "tattooing 319\n",
            "tattooing 319\n",
            "teaching 126\n",
            "teaching 126\n",
            "teaching 126\n",
            "tearing 124\n",
            "tearing 124\n",
            "tearing 124\n",
            "telephoning 242\n",
            "telephoning 242\n",
            "telephoning 242\n",
            "750/834\n",
            "throwing 268\n",
            "throwing 268\n",
            "throwing 268\n",
            "tickling 334\n",
            "tickling 334\n",
            "tickling 334\n",
            "towing 241\n",
            "towing 241\n",
            "towing 241\n",
            "trimming 63\n",
            "trimming 63\n",
            "trimming 63\n",
            "tripping 149\n",
            "tripping 149\n",
            "tripping 149\n",
            "tuning 19\n",
            "tuning 19\n",
            "tuning 19\n",
            "turning 248\n",
            "turning 248\n",
            "turning 248\n",
            "twisting 15\n",
            "twisting 15\n",
            "twisting 15\n",
            "tying 236\n",
            "tying 236\n",
            "tying 236\n",
            "typing 338\n",
            "typing 338\n",
            "typing 338\n",
            "unloading 297\n",
            "unloading 297\n",
            "unloading 297\n",
            "unpacking 155\n",
            "unpacking 155\n",
            "unpacking 155\n",
            "vacuuming 87\n",
            "vacuuming 87\n",
            "vacuuming 87\n",
            "waking 230\n",
            "waking 230\n",
            "waking 230\n",
            "walking 179\n",
            "walking 179\n",
            "walking 179\n",
            "washing 39\n",
            "washing 39\n",
            "washing 39\n",
            "watering 121\n",
            "watering 121\n",
            "800/834\n",
            "watering 121\n",
            "waving 270\n",
            "waving 270\n",
            "waving 270\n",
            "waxing 252\n",
            "waxing 252\n",
            "waxing 252\n",
            "weeding 158\n",
            "weeding 158\n",
            "weeding 158\n",
            "wetting 173\n",
            "wetting 173\n",
            "wetting 173\n",
            "whistling 302\n",
            "whistling 302\n",
            "whistling 302\n",
            "winking 40\n",
            "winking 40\n",
            "winking 40\n",
            "working 48\n",
            "working 48\n",
            "working 48\n",
            "wrapping 172\n",
            "wrapping 172\n",
            "wrapping 172\n",
            "wrestling 332\n",
            "wrestling 332\n",
            "wrestling 332\n",
            "writing 141\n",
            "writing 141\n",
            "writing 141\n",
            "yawning 329\n",
            "yawning 329\n",
            "yawning 329\n",
            "\n",
            "Time elapsed:: 0.2224s (~ 0.00s per file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'speaking'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "categories[327]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8faa9a8V1w6v"
      },
      "source": [
        "Stack together and save to csv as: \\\\\n",
        "  `category, fname, mif_idx`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soWUKoct99Yd",
        "outputId": "33037a38-b7e8-4975-d644-566eae80c067"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Additional information comparing pandas data frame and pickle files see also at the end.\n",
        "\n",
        "df = pd.DataFrame(l_mifs, columns=['category', 'fname', 'mif_idx', 'softmax[category]'])\n",
        "print(df)\n",
        "df.to_csv(path_prefix / 'temp/mifs.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     category                   fname  mif_idx  softmax[category]\n0      aiming  yt-0gwUV4Ze-Hs_390.mp4       58           0.607241\n1      aiming   yt-0qYbATyHm2A_59.mp4        4           0.068653\n2      aiming  yt-2yYb3iQCivw_130.mp4       41           0.407697\n3      aiming   yt-chT_6aIyhD4_47.mp4        8           0.019289\n4      aiming  yt-fG9wZzs4jis_124.mp4        1           0.354655\n5      aiming   yt-fM2iXUuaP7U_48.mp4        0           0.051184\n6      aiming   yt-iVSy96zolvw_23.mp4       43           0.270948\n7  applauding    yt-06tUmXhgnSY_4.mp4       48           0.062576\n8  applauding    yt-A70byjNkwdA_4.mp4        0           0.026515\n9  applauding   yt-E14-2TmbCD8_12.mp4        1           0.540867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTsOB6_wfD6-"
      },
      "source": [
        "#### Testing\n",
        "\n",
        "Using a previously computed dictionary of softmax values, check if same values were collected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhKjMSWJfDaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a319fd9a-289c-4bfa-fbe2-902c3f8579e9"
      },
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# AB: A bit more info considering the pickle file and where it comes from (was this in '01.mif_extraction_exploratory' initially?)\n",
        "\n",
        "dict_path = path_prefix / 'temp/ResNet50_MiTv1_accuracies_per_category.pkl'\n",
        "# Load from file\n",
        "f = open(dict_path, 'rb')\n",
        "accuracies_per_category = pickle.load(f)\n",
        "\n",
        "l_categories = categories\n",
        "\n",
        "# AB: Example ./aiming/yt-0gwUV4Ze-Hs_390.mp4\n",
        "\n",
        "category_name = 'aiming'\n",
        "video_fname = 'yt-0gwUV4Ze-Hs_390.mp4'\n",
        "\n",
        "per_frame_accuracies = np.array(accuracies_per_category[category_name][video_fname])\n",
        "\n",
        "print(f'\\t{video_fname} : Max/Min accuracy at frame:' \\\n",
        "f' {np.argmax(per_frame_accuracies)}/{np.argmin(per_frame_accuracies)}' \\\n",
        "f' with value: {per_frame_accuracies[np.argmax(per_frame_accuracies)]}' \\\n",
        "f' / {per_frame_accuracies[np.argmin(per_frame_accuracies)]}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tyt-0gwUV4Ze-Hs_390.mp4 : Max/Min accuracy at frame: 58/23 with value: [0.60724086] / [0.083207]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  category                   fname  mif_idx  softmax[category]\n",
              "0   aiming  yt-0gwUV4Ze-Hs_390.mp4       58           0.607241"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>fname</th>\n      <th>mif_idx</th>\n      <th>softmax[category]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aiming</td>\n      <td>yt-0gwUV4Ze-Hs_390.mp4</td>\n      <td>58</td>\n      <td>0.607241</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "df.loc[lambda df: df['fname'] == video_fname]"
      ]
    },
    {
      "source": [
        "## ALL classes & ALL frames\n",
        "\n",
        "**Output**\n",
        "\n",
        "    ```\n",
        "    {\n",
        "        category_i : {\n",
        "            video_j : [list of per-frame accuracies for all categories]\n",
        "        }\n",
        "    }\n",
        "    ```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Time duration:\n",
        "* i5-7400 CPU @ 3.00GHz; 16GB RAM; No CUDA: **>15h**\n",
        "* Google Colab + GPU (CUDA) acceleration: **3-5h**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.nn import functional as F\n",
        "\n",
        "accuracies_per_category = {}\n",
        "z = 1 # For verbose\n",
        "\n",
        "time_init = time.time()\n",
        "\n",
        "# Iterate over categories in path_prefix\n",
        "for category_name in list(d_files_per_category.keys()):\n",
        "    print(f'{category_name} {z}/{len(list(d_files_per_category.keys()))}'); z += 1\n",
        "    \n",
        "    # Define empty dictionary entry for current category\n",
        "    accuracies_per_category[category_name] = {}\n",
        "\n",
        "    # Iterate over files in cateogory_path\n",
        "    for video in list(d_files_per_category[category_name]):\n",
        "        # verbose\n",
        "        print('\\t', video)\n",
        " \n",
        "        video_fname = d_files_per_category[category_name][video]\n",
        "        \n",
        "        # Load video with Decord.VideoReader\n",
        "        vr = VideoReader(video_fname, ctx=cpu(0))\n",
        "        frame_id_list = range(0, len(vr), 1)\n",
        "        video_frames = vr.get_batch(frame_id_list)\n",
        "\n",
        "        pred_accuracies = []\n",
        "        true_category = category_name\n",
        "\n",
        "        start_time = time.time()\n",
        "        # Iterate through frames\n",
        "        for i in range(video_frames.shape[0]):\n",
        "            # Load frame\n",
        "            frame = video_frames.asnumpy()[i]\n",
        "\n",
        "            # Transform\n",
        "            if with_cuda:\n",
        "                input = transformation(frame).to('cuda')\n",
        "            else:\n",
        "                input = transformation(frame)\n",
        "            \n",
        "            # Classification:\n",
        "            logit = resnet50_moments.forward(input.unsqueeze(0))\n",
        "            h_x = F.softmax(logit, 1).data.squeeze()\n",
        "            probs, idx = h_x.sort(0, True)\n",
        "\n",
        "            # Save accuracies for the all classes (TYPE #2)\n",
        "            pred_accuracies.append(probs.cpu().numpy())\n",
        "        \n",
        "        # Calculate avg duration per frame\n",
        "        end_time = time.time()\n",
        "        print('\\tAvg duration per frame: %4.4f seconds.' % ((end_time - start_time)/(video_frames.shape[0])))\n",
        "        \n",
        "        # Add computed list of accuracies as entry in the output dictionary\n",
        "        accuracies_per_category[category_name][video] = pred_accuracies\n",
        "\n",
        "time_final = time.time()\n",
        "duration = time_final-time_init\n",
        "print(f'\\nTime elapsed: {duration:.2f}s (~ {duration/z:.2f}s per file)')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dictionary to pickle file\n",
        "\n",
        "# Difference: pandas data frame || pickle file\n",
        "# The pickle (.pkl) file is different form the pandas dataframe used earlier.\n",
        "\n",
        "# Where the dataframe contains the foruc columns:\n",
        "# category fname mif_idx  softmax[category]\n",
        "\n",
        "# the pickle file contains the accuracy per category for all files (massive data)\n",
        "# additionally it contains the nested structure of the data\n",
        "# (the mif indices could also restored from the larger pickle file again,\n",
        "# but not the other way round)\n",
        "\n",
        "import pickle\n",
        "dict_path = path_prefix / 'temp/accuracies_per_category_mitv1_fps-25.pkl'\n",
        "\n",
        "if dict_path.is_file():\n",
        "    print(f'File {dict_path} already exists. Change dict_path and re-run cell!')\n",
        "else:\n",
        "    with open(dict_path, 'wb') as f:\n",
        "        pickle.dump(accuracies_per_category, f)\n",
        "        f.close()\n",
        "    print(f'Saved at {dict_path.absolute()}')"
      ]
    },
    {
      "source": [
        "Difference: pandas data frame || pickle file\n",
        "\n",
        "The pickle (.pkl) file is different form the pandas dataframe used earlier.\n",
        "\n",
        "Where the dataframe contains the foruc columns:\n",
        "category fname mif_idx  softmax[category]\n",
        "\n",
        "the pickle file contains the accuracy per category for all files (massive data) additionally it contains the nested structure of the data\n",
        "(the mif indices could also restored from the larger pickle file again, but not the other way round)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}